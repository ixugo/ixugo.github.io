---
title: 太多连接数
description: 
date: 2024-08-22
slug: 
image: 
draft: true
categories:
    - 后端
tags:
---

## 背景

遇到线上服务提示 `too many open files`，可以通过 `/proc/<进程 ID>/limits` 来查看限制的进程最大连接数，一般是 4096，可以简单的通过修改最大打开文件数量来解决问题，但知其然，更要知其所以然。

## 分析

通过 `lsof -p <进程 ID>`  列出以下信息，从下图中可以看到好几千连接数都是这个。

补充一下图所示分别表示什么意思

+ PID: 进程 ID
+ USER: 启动进程的用户
+ FD: 文件描述符
+ TYPE: 文件类型，此处的 sock 表示 套接字。
+ DEVICE: 设备号
+ SIZE/OFF: 文件大小或偏移量
+ NODE: 文件的 iode 节点号码
+ NAME: 文件名称或路径/ socket 的详细信息

`What Fuck?`，在这里得知了 2 个条件。

1. 90% 的连接数都是同类
2. 占用竟然全是 TCPv6 的 sock 连接，根据业务判断，不可能同时有这么多连接数，显然是资源泄漏。

![image-20240822104718879](http://img.golang.space/img-1724294839070.png)

通过 ` netstat -anp | grep 16683` 查看连接情况。

由下图可知没有几千个 tcp6 的链接，但 sock 文件描述符还在。

![image-20240822110237683](http://img.golang.space/img-1724295757844.png)

基于此，大概问题范围被圈定了，在我们业务中有 2 个地方，会产生这些大量的连接。

登录网页端，在 2 个业务分别操作，检查占用文件描述符数量是否上涨，最终定位最大嫌疑。

首先可以确定 socket 是关闭的，所以 ss/netstat 查询不到信息，但文件描述符一直占用，大概率是本地服务还在处理那个关闭的 socket (猜测)，检查对应业务的协程数量是否与文件描述符数量相差无几。

## 验证分析

